{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3eb231",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e230803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPTNeoForCausalLM,\n",
    "    GPT2TokenizerFast\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44d98b",
   "metadata": {},
   "source": [
    "## Logging setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename=\"autocomplete.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10453808",
   "metadata": {},
   "source": [
    "## Load Models & Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62b1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 2302.53it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|██████████| 160/160 [00:00<00:00, 2197.20it/s, Materializing param=transformer.wte.weight]                         \n",
      "GPTNeoForCausalLM LOAD REPORT from: EleutherAI/gpt-neo-125M\n",
      "Key                                                   | Status     |  | \n",
      "------------------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 2, 4, 6, 8, 10}.attn.attention.bias | UNEXPECTED |  | \n",
      "transformer.h.{0...11}.attn.attention.masked_bias     | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"gpt2\": {\n",
    "        \"model\": GPT2LMHeadModel.from_pretrained(\"gpt2\"),\n",
    "        \"tokenizer\": GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    },\n",
    "    \"gpt_neo\": {\n",
    "        \"model\": GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\"),\n",
    "        \"tokenizer\": GPT2TokenizerFast.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# GPT models do not have a pad token, so set pad = eos\n",
    "for entry in models.values():\n",
    "    tokenizer = entry[\"tokenizer\"]\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    entry[\"model\"].eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864454d",
   "metadata": {},
   "source": [
    "## Stopping Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4044e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_at_sentence(text):\n",
    "    \"\"\"\n",
    "    Stop generation at the first end-of-sentence token.\n",
    "    \"\"\"\n",
    "    if \".\" in text:\n",
    "        return text[: text.find(\".\") + 1]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997f256",
   "metadata": {},
   "source": [
    "## Autocomplete Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc3907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete(\n",
    "    prompt,\n",
    "    model_name,\n",
    "    num_outputs=3,\n",
    "    max_new_tokens=40,\n",
    "    top_k=30,\n",
    "    temperature=0.45\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to generate sentence completions using a specified model and parameters.\n",
    "    \"\"\"\n",
    "    entry = models[model_name]\n",
    "    model = entry[\"model\"]\n",
    "    tokenizer = entry[\"tokenizer\"]\n",
    "\n",
    "    # Log the generation parameters with the model name and prompt\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(f\"MODEL: {model_name}\")\n",
    "    logger.info(f\"PROMPT: {prompt}\")\n",
    "    logger.info(\n",
    "        f\"PARAMS | top_k={top_k}, temperature={temperature}, \"\n",
    "        f\"max_new_tokens={max_new_tokens}\"\n",
    "    )\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_enc = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Generate outputs with the specified parameters\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_enc.input_ids,\n",
    "        attention_mask=input_enc.attention_mask,\n",
    "        do_sample=True,\n",
    "        top_k=top_k,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=num_outputs,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and log each generated output\n",
    "    completions = []\n",
    "    for i, out in enumerate(outputs, start=1):\n",
    "        text = tokenizer.decode(out, skip_special_tokens=True)\n",
    "        text = stop_at_sentence(text)\n",
    "        completions.append(text)\n",
    "        logger.info(f\"OUTPUT {i}: {text}\")\n",
    "\n",
    "    return completions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bced14a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Computes perplexity (Fluency).\n",
    "    \"\"\"\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=enc.input_ids,\n",
    "            attention_mask=enc.attention_mask,\n",
    "            labels=enc.input_ids\n",
    "        )\n",
    "\n",
    "    return math.exp(outputs.loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_2(sentences):\n",
    "    \"\"\"\n",
    "    Computes Distinct-2 (bigram diversity).\n",
    "    \"\"\"\n",
    "    bigrams = []\n",
    "\n",
    "    for s in sentences:\n",
    "        words = s.split()\n",
    "        bigrams.extend(\n",
    "            [tuple(words[i:i+2]) for i in range(len(words) - 1)]\n",
    "        )\n",
    "    return len(set(bigrams)) / max(1, len(bigrams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8f99f",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf97bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The team is planning to\",\n",
    "    \"AI can help improve\",\n",
    "    \"She decided to invest in\",\n",
    "    \"Machine learning is used for\",\n",
    "    \"I am thinking of traveling to\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f619e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "\n",
    "for model_name, entry in models.items():\n",
    "    model = entry[\"model\"]\n",
    "    tokenizer = entry[\"tokenizer\"]\n",
    "\n",
    "    ppl_scores = []\n",
    "    diversity_scores = []\n",
    "\n",
    "    # Generate outputs for each prompt and evaluate them\n",
    "    for prompt in prompts:\n",
    "        outputs = autocomplete(\n",
    "            prompt=prompt,\n",
    "            model_name=model_name\n",
    "        )\n",
    "\n",
    "        # Perplexity\n",
    "        avg_ppl = sum(\n",
    "            calculate_perplexity(o, model, tokenizer)\n",
    "            for o in outputs\n",
    "        ) / len(outputs)\n",
    "\n",
    "        ppl_scores.append(avg_ppl)\n",
    "\n",
    "        # Diversity\n",
    "        diversity_scores.append(distinct_2(outputs))\n",
    "\n",
    "    # Store average metrics for the model\n",
    "    evaluation_results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Avg Perplexity\": round(sum(ppl_scores) / len(ppl_scores), 2),\n",
    "        \"Avg Distinct-2\": round(sum(diversity_scores) / len(diversity_scores), 2)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19286baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg Perplexity</th>\n",
       "      <th>Avg Distinct-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>13.84</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt_neo</td>\n",
       "      <td>12.14</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Avg Perplexity  Avg Distinct-2\n",
       "0     gpt2           13.84            0.69\n",
       "1  gpt_neo           12.14            0.70"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(evaluation_results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e393b1",
   "metadata": {},
   "source": [
    "#### Lower perplexity and higher Distinct scores indicate better performance; based on this trade-off, the GPT-Neo model was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19ab2b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. AI can help improve the quality of the medical care it provides, but it also can help improve the efficiency of the hospital.\n",
      "2. AI can help improve the quality of life of patients with diabetes by improving the insulin sensitivity and insulin secretion.\n",
      "3. AI can help improve the quality of life of people with dementia.\n"
     ]
    }
   ],
   "source": [
    "final_model = \"gpt_neo\"\n",
    "prompt = \"AI can help improve\"\n",
    "\n",
    "results = autocomplete(prompt, final_model)\n",
    "\n",
    "for i, r in enumerate(results, start=1):\n",
    "    print(f\"{i}. {r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43fbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pega",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
